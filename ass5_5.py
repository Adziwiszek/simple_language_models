# -*- coding: utf-8 -*-
"""ass5_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GiVctmUuyzDlc4e7R4ogeueOUPxaOClE
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import requests
from collections import Counter
import string
import os

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim
import matplotlib.pyplot as plt
import nltk
import wandb

from my_secrets import wandb_key

wandb_proj_name = 'Language_model'
wandb.login(key=wandb_key)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#data_path = "/content/drive/MyDrive/Colab/"
script_dir = os.path.dirname(os.path.abspath(__file__))
glove_dir = os.path.join(script_dir, "data")
glove_path = os.path.join(glove_dir, "glove.6B.100d.txt")
#!ls "$glove_dir"

#!wget -P {glove_dir} http://nlp.stanford.edu/data/glove.6B.zip
#!unzip -q {glove_dir}/glove.6B.zip -d {glove_dir}

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('gutenberg')

"""# 1. Dataset preparation"""

def extract_gutenberg_text(full_text):
    start_marker = "*** START"
    end_marker = "*** END"

    start = full_text.find(start_marker)
    end = full_text.find(end_marker)

    if start != -1 and end != -1:
        start = full_text.index("\n", start) + 1
        return full_text[start:end].strip()
    else:
        print("Can't find markers, returning whole text.")
        return full_text

def get_raw_text(link="https://gutenberg.org/cache/epub/1513/pg1513.txt",
                 text_type='gutenberg'):
    '''Downloads text from given link and preprocesses it, deleting useless info.'''
    r = requests.get(link)
    r.encoding = 'utf-8'
    data = r.text

    if text_type == 'gutenberg':
        return extract_gutenberg_text(data).lower()
    else:
        raise ValueError("Wrong type of downloaded text!")

def preprocess_text(raw_text: str, keep_words: int = 10000):
    '''Preprocesses raw text and converts it into tokens.
    raw_text - text that will be tokenized
    keep_words - how many words to keep for training
    Returns:
    tokens - all unique tokens that appear in the text
    raw_tokens - whole text, but split into tokens
    '''
    # tokenize and preprocess words (easier to remove punctuation from tokens)
    tokens = nltk.word_tokenize(raw_text)
    tokens = list(filter(lambda token: token not in string.punctuation, tokens))
    #raw_tokens = list(map(lambda w: w.lower(), raw_tokens))
    print(f'raw tokens: {len(tokens)}')
    counted_tokens = Counter(tokens)
    most_tokens = list(counted_tokens.most_common(keep_words))
    print(f'most freq tokens: {len(most_tokens)}')
    special_tokens = ['<BOS>', '<EOS>', '<UNK>', '<PAD>']
    vocab = list(map(lambda p: p[0], most_tokens)) + special_tokens
    print(f'tokens: {len(vocab)}')
    return vocab, tokens

def create_embedding_layer(tokens):
    '''Takes tokens, loads glove embeddings from file, and populates embedding matrix'''
    # create mappings from words to ids
    word2id = {w: i for i, w in enumerate(tokens)}
    id2word = {i: w for w, i in word2id.items()}
    # load glove embeddings
    def load_glove_embeddings(glove_path):
        embeddings = {}
        with open(glove_path, 'r', encoding='utf8') as f:
            for line in f:
                parts = line.strip().split()
                word = parts[0]
                vector = np.array(parts[1:], dtype=np.float32)
                embeddings[word] = vector
        return embeddings
    glove_embeddings = load_glove_embeddings(glove_path)
    embedding_dim = len(next(iter(glove_embeddings.values())))
    # fill embedding matrix
    embedding_matrix = np.zeros((len(word2id), embedding_dim), dtype=np.float32)
    for word, idx in word2id.items():
        embedding_matrix[idx] = glove_embeddings.get(word, np.random.normal(scale=0.6, size=(embedding_dim,)))
    embedding_tensor = torch.tensor(embedding_matrix)
    embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)
    return embedding_layer, word2id, id2word

'''Downloads data, loads embeddings, creates dictionaries'''
raw_text = get_raw_text(link='https://gutenberg.org/cache/epub/100/pg100.txt')
vocab, tokens = preprocess_text(raw_text)
embedding_layer, word2id, id2word = create_embedding_layer(vocab)

token_ids = [word2id.get(tok, word2id["<UNK>"]) for tok in tokens]

seq_len = 20
X, Y = [], []

for i in range(len(token_ids) - seq_len):
    x_seq = token_ids[i:i+seq_len]
    y_seq = token_ids[i+1:i+seq_len+1]
    X.append(x_seq)
    Y.append(y_seq)

X = torch.tensor(X)
Y = torch.tensor(Y)

batch_size = 128
dataset = TensorDataset(X, Y)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""# WandB setup"""

def setup_wandb(
    model,
    num_epochs=10,
    batch_size=16,
    device='cpu',
    lr=0.001,
    tags=[],
    ):
    wandb_config = {
        "lr": lr,
        "max_epochs": num_epochs,
        "model_type": model,
        #"lr_scheduler": lr_scheduler,
        "dataset": "Whole Shakespear",
        "criterion": "CrossEntropyLoss",
        "optimizer": "Adam",
        "batch_size": batch_size,
        "device": device,
    }
    wandb.init(
        project=wandb_proj_name,
        config=wandb_config,
        tags=tags,
    )
    return wandb_config

def finish_wandb():
    wandb.save("model.pt")
    wandb.finish()

"""# 2. Model

## RNN layer
"""

class RNNLayer(nn.Module):
    def __init__(self, num_inputs, num_hidden, sigma=0.01):
        super().__init__()
        self.sigma = sigma
        self.num_inputs = num_inputs
        self.num_hidden = num_hidden
        self.W_xh = nn.Parameter(
            torch.randn(num_inputs, num_hidden) * sigma
        )
        self.W_hh = nn.Parameter(
            torch.randn(num_hidden, num_hidden) * sigma
        )
        self.b_h = nn.Parameter(torch.zeros(num_hidden))

    def forward(self, inputs, state=None):
        if state is None:
            # Initial state with shape: (batch_size, num_hiddens)
            state = torch.zeros(
                (inputs.shape[1], self.num_hidden),
                device=inputs.device
            )
        else:
            state, = state
        outputs = []
        for X in inputs:
            state = torch.tanh(
                torch.matmul(X, self.W_xh) +
                torch.matmul(state, self.W_hh) + self.b_h)
            outputs.append(state)
        #print(type(outputs), len(outputs))
        outputs = torch.stack(outputs)
        return outputs, state

"""## LSTM layer"""

class LSTMLayer(nn.Module):
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.num_inputs = num_inputs
        self.num_hiddens = num_hiddens
        self.sigma = sigma

        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)
        triple = lambda: (init_weight(num_inputs, num_hiddens),
                          init_weight(num_hiddens, num_hiddens),
                          nn.Parameter(torch.zeros(num_hiddens)))
        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate
        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate
        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate
        self.W_xc, self.W_hc, self.b_c = triple()  # Input node

    def forward(self, inputs, H_C=None):
        if H_C is None:
            # Initial state, shape: (batch_size, num_hiddes)
            H = torch.zeros((inputs.shape[1], self.num_hiddens),
                            device=inputs.device)
            C = torch.zeros((inputs.shape[1], self.num_hiddens),
                            device=inputs.device)
        else:
            H, C = H_C
        outputs = []
        for X in inputs:
            I = torch.sigmoid(torch.matmul(X, self.W_xi) +
                              torch.matmul(H, self.W_hi) + self.b_i)
            F = torch.sigmoid(torch.matmul(X, self.W_xf) +
                              torch.matmul(H, self.W_hf) + self.b_f)
            O = torch.sigmoid(torch.matmul(X, self.W_xo) +
                              torch.matmul(H, self.W_ho) + self.b_o)
            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +
                                 torch.matmul(H, self.W_hc) + self.b_c)
            C = F * C + I * C_tilde
            H = O * torch.tanh(C)
            outputs.append(H)
        outputs = torch.stack(outputs)
        return outputs, (H, C)

"""GRU layer"""
class GRULayer(nn.Module):
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.num_inputs = num_inputs
        self.num_hiddens = num_hiddens
        self.sigma = sigma

        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)
        triple = lambda: (init_weight(num_inputs, num_hiddens),
                          init_weight(num_hiddens, num_hiddens),
                          nn.Parameter(torch.zeros(num_hiddens)))

        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate
        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate
        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state

    def forward(self, inputs, H=None):
        if H is None:
            # Initial state with shape: (batch_size, num_hiddens)
            H = torch.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
        outputs = []
        for X in inputs:
            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +
                            torch.matmul(H, self.W_hz) + self.b_z)
            R = torch.sigmoid(torch.matmul(X, self.W_xr) +
                            torch.matmul(H, self.W_hr) + self.b_r)
            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +
                               torch.matmul(R * H, self.W_hh) + self.b_h)
            H = Z * H + (1 - Z) * H_tilde
            outputs.append(H)
        outputs = torch.stack(outputs)
        return outputs, H

"""## Model itself"""

class RNNModel(nn.Module):
    def __init__(self, embedding_layer, hidden_size, layer_type='rnn'):
        super().__init__()
        vocab_size, embedding_dim = embedding_layer.weight.shape
        self.vocab_size = vocab_size
        self.embedding = embedding_layer
        if layer_type == 'rnn':
            self.rnn = RNNLayer(num_inputs=embedding_dim, num_hidden=hidden_size)
        elif layer_type == 'lstm':
            self.rnn = LSTMLayer(num_inputs=embedding_dim, num_hiddens=hidden_size)
        elif layer_type == 'gru':
            self.rnn = GRULayer(num_inputs=embedding_dim, num_hiddens=hidden_size)
        else:
            raise ValueError(f"No layer {layer_type}. Pick one from: 'rnn'")
        self.output_layer = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_seq, state=None):
        # input_seq: (seq_len, batch_size)
        embeddings = self.embedding(input_seq)  # -> (seq_len, batch_size, embedding_dim)
        outputs, state = self.rnn(embeddings, state)  # -> (seq_len, batch_size, hidden_size)
        logits = self.output_layer(outputs)  # -> (seq_len, batch_size, vocab_size)
        return logits, state

    def predict(self, prefix, num_preds):
        prefix = nltk.word_tokenize(prefix)
        state, outputs = None, [word2id[prefix[0]]]
        temperature = 8

        for i in range(len(prefix) + num_preds - 1):
            x = torch.tensor([[outputs[-1]]], device=device)
            embs = self.embedding(x)
            rnn_outputs, state = self.rnn(embs, state)
            if i < len(prefix) - 1:  # Warm-up period
                outputs.append(word2id.get(prefix[i + 1], word2id['<UNK>']))
            else:  # Predict num_preds steps
                logits = self.output_layer(rnn_outputs[-1].unsqueeze(0))
                logits = logits[0, -1] / temperature  # <-- SKALUJEMY logity
                probs = torch.softmax(logits, dim=0)
                next_token = torch.multinomial(probs, num_samples=1).item()
                outputs.append(next_token)
                #outputs.append(int(Y.argmax(axis=2).reshape(1)))
        prediction = ' '.join([id2word[i] for i in outputs])
        return prediction

    def predict_mass(self, prefix, num_preds, num_trials):
        all_preds = []
        for _ in range(num_trials):
            all_preds.append(self.predict(prefix, num_preds))
        return all_preds

"""# Training"""

def train(
    model,
    dataloader,
    num_epochs=4,
    log_every=3000,
    lr=0.001,
    ):
    #model = RNNModel(embedding_layer, hidden_size=128).to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses = []
    best_loss = float("inf")

    for epoch in range(num_epochs):
        total_loss = 0
        batch_count = 0

        for x, y in dataloader:
            x, y = x.to(device), y.to(device)  # X: (seq_len, batch), Y: (seq_len, batch)

            optimizer.zero_grad()
            logits, _ = model(y)  # logits: (seq_len, batch_size, vocab_size)

            # PrzeksztaÅ‚cenie do (seq_len * batch, vocab_size) i (seq_len * batch)
            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            batch_count += 1
            if batch_count % log_every == 0:
                print(f'batch {batch_count}, avg loss: {total_loss/batch_count}')

            wandb.log({
                "train_loss": total_loss/batch_count,
            })

        avg_loss = total_loss / batch_count

        # save best model
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "best_model.pth")

            artifact = wandb.Artifact("best-model", type="model")
            artifact.add_file("best_model.pth")
            wandb.log_artifact(artifact)

        train_losses.append(avg_loss)
        print(f"Epoch {epoch+1}, Loss: {avg_loss}")

    '''
    plt.plot(train_losses)
    plt.title("Training loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()
    '''

def perform_experiment(
    model_type,
    num_epochs=4,
    hidden_size=128,
    lr=0.001,
    tags=[],
):
    model = RNNModel(
        embedding_layer,
        hidden_size=hidden_size,
        layer_type=model_type,
    ).to(device)
    # start wandb
    setup_wandb(
        model=model_type,
        num_epochs=num_epochs,
        batch_size=batch_size,
        device=device,
        lr=lr,
        tags=[],
    )
    # train model
    train(
        model,
        dataloader,
        num_epochs=num_epochs,
        lr=lr,
        log_every=1000,
    )
    # generating predictions
    '''prefix = 'oh romeo i '
    preds = model.predict_mass(prefix, num_preds=8, num_trials=5)
    wandb.log({
        'predictions': preds,
    })
    print(f'model predictions for: {prefix}\n{preds}')'''
    
    columns = ["Prompt", "Generation 1", "Generation 2", "Generation 3", "Generation 4"]
    table = wandb.Table(columns=columns)

    prompts = ["oh romeo my romeo ", "i think we should ", "let us take this moment "]
    for prompt in prompts:
        row = [prompt]
        for _ in range(4):
            text = model.predict(prompt, 7)
            row.append(text)
        table.add_data(*row)

    wandb.run.summary["final_predictions"] = table
    # finishing wanbd_run
    finish_wandb()

    return model

models_to_test = ['rnn', 'lstm', 'gru']
for model_type in models_to_test:
    model = perform_experiment(
        model_type,
        num_epochs=4,
        hidden_size=128,
        lr=0.001,
        tags=[],
    )

    print(model.predict('romeo i want ',  10))
